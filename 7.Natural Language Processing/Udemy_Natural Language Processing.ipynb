{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Udemy_Natural Language Processing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPAA+FhoJZGYAXpFyUnyed8"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"OjBPEJTDvPwQ","colab_type":"code","outputId":"b5e42edf-dd44-4285-fd48-9034768f5163","executionInfo":{"status":"ok","timestamp":1582691165134,"user_tz":-330,"elapsed":27072,"user":{"displayName":"RAJ SHAH","photoUrl":"","userId":"02247319882991003562"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["#Natural Language Processing\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-98jAIl-vXfV","colab_type":"code","colab":{}},"source":["#Importing Libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWozAc2avZtT","colab_type":"code","colab":{}},"source":["#Importing Dataset\n","dataset = pd.read_csv('gdrive/My Drive/Data/Restaurant_Reviews.tsv',delimiter = '\\t', quoting = 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7_8qcUDgxKo9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m6Hk1sLa1uCv","colab_type":"text"},"source":["#Cleaning the sets\n","**Steps**\n","\n","1.   Keep only alphabets\n","2.   Convert to lowercase\n","3.   Convert string to list\n","4.   Remove stopwords\n","5.   Stemming (e.g. Convert loved, loving to love)\n","6.   Convert list back to string"]},{"cell_type":"code","metadata":{"id":"Y0bFFHMWxRTU","colab_type":"code","outputId":"04da7819-1a62-45cc-f1aa-0c10bd632eb8","executionInfo":{"status":"ok","timestamp":1582691170890,"user_tz":-330,"elapsed":5742,"user":{"displayName":"RAJ SHAH","photoUrl":"","userId":"02247319882991003562"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#Cleaning the sets\n","import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer \n","\n","corpus = [] #Corpus is a collection of texts of same type\n","for i in range(0, 1000):\n","  review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) #Kept only alphabets\n","  review = review.lower() #to lower case\n","  review = review.split() #string to list\n","  ps = PorterStemmer()\n","  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] #Removing stopwords like the, a, this\n","  review = ' '.join(review)\n","  corpus.append(review)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bkOoKqeE6OiI","colab_type":"code","outputId":"7add5f66-dd93-45a7-9245-0f26d552d2cd","executionInfo":{"status":"ok","timestamp":1582691174706,"user_tz":-330,"elapsed":2422,"user":{"displayName":"RAJ SHAH","photoUrl":"","userId":"02247319882991003562"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# Creating the Bag of Words Model\n","# Tokenization is process of taking all different words of the review and creating column for each words\n","from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer(max_features = 1500)\n","X = cv.fit_transform(corpus).toarray()\n","y = dataset.iloc[:, 1].values\n","print(X)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," ...\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c7mKqvcn9xle","colab_type":"code","colab":{}},"source":["#We can use dimensionality reduction to reduce the number of dimensions\n","\n","#Split dataset into train-test\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n","\n","#FeatureScaling\n","from sklearn.preprocessing import StandardScaler\n","sc_X=StandardScaler()\n","X_train = sc_X.fit_transform(X_train)\n","X_test = sc_X.transform(X_test)\n","\n","#Fitting Naive Bayes to the dataset, Accuracy = 71%\n","'''\n","from sklearn.naive_bayes import GaussianNB\n","classifier = GaussianNB()\n","'''\n","\n","#Fitting Decision Trees to the dataset, Accuracy = 71%\n","'''\n","from sklearn.tree import DecisionTreeClassifier\n","classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0) \n","'''\n","\n","#Fitting Random Forest Classifier to the dataset, Accuracy = 72%\n","\n","from sklearn.ensemble import RandomForestClassifier\n","classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0) \n","\n","\n","#Fitting Logistic Regression to the Training set, Accuracy = 75%\n","'''\n","from sklearn.linear_model import LogisticRegression\n","classifier = LogisticRegression(random_state = 0)\n","'''\n","#Fitting k Nearest Neighbors to training set, Accuracy = 65%\n","'''\n","from sklearn.neighbors import KNeighborsClassifier\n","classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n","'''\n","\n","#Fitting SVM to the dataset, Accuracy = 69%\n","'''\n","from sklearn.svm import SVC\n","classifier = SVC(kernel = 'linear', random_state = 0)\n","'''\n","\n","#Fitting kernel SVM to the dataset, Accuracy = 73%\n","'''\n","from sklearn.svm import SVC\n","classifier = SVC(kernel = 'rbf', random_state = 0)\n","'''\n","\n","classifier.fit(X_train, y_train)\n","\n","#Predicting test set results\n","y_pred = classifier.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCDwgwF5IF1N","colab_type":"code","outputId":"d53081d1-3a8f-417c-c29c-7b3607b02a2c","executionInfo":{"status":"ok","timestamp":1582016633309,"user_tz":-330,"elapsed":895,"user":{"displayName":"RAJ SHAH","photoUrl":"","userId":"02247319882991003562"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["#Making the Confusion Matrix \n","'''     Predicted\n","          0   1\n","Actual 0  TN  FP\n","Actual 1  FN  TP\n","'''\n","from sklearn.metrics import confusion_matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","tn = cm[0][0]\n","fp = cm[0][1]\n","fn = cm[1][0]\n","tp = cm[1][1]\n","accuracy = (tp+tn)/(tp+tn+fp+fn)\n","precision = tp/(tp+fp)\n","recall = tp/(tp+fn)\n","f1 = 2*precision*recall/(precision+recall)\n","print('Accuracy is',accuracy)\n","print('Precision is',precision)\n","print('Recall is',recall)\n","print('F1 Score is',f1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[87 10]\n"," [46 57]]\n","Accuracy is 0.72\n","Precision is 0.8507462686567164\n","Recall is 0.5533980582524272\n","F1 Score is 0.6705882352941177\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YEYF0vKILUbJ","colab_type":"code","outputId":"f9183ac7-0523-44f7-9ac5-a517e3568651","executionInfo":{"status":"ok","timestamp":1582016597156,"user_tz":-330,"elapsed":4882,"user":{"displayName":"RAJ SHAH","photoUrl":"","userId":"02247319882991003562"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["#Combining three models to get more accuracy\n","Model used: RandomForest, DecisionTree and NaiveBayes\n","\n","'''\n","def build_ensemble_classifier(X, y):\n","    classifier1 = RandomForestClassifier()\n","    classifier1.fit(X, y) \n","    \n","    classifier2 = DecisionTreeClassifier()\n","    classifier2.fit(X, y)\n"," \n","    classifier3 = GaussianNB()\n","    classifier3.fit(X, y)\n","    \n","    return [classifier1, classifier2, classifier3]\n"," \n","def ensemble_classifier_predict(X, classifiers):\n","    results = [classifier.predict(X) for classifier in classifiers]\n","    processed_results = []\n","    \n","    for i in range(len(results[0])):\n","        votes = []\n","        \n","        for j in range(len(results)):\n","            votes.append(results[j][i])\n","            \n","        processed_results.append(decide_voting(votes))\n","                \n","    return np.array(processed_results)\n"," \n","def decide_voting(votes):\n","    zeros = 0\n","    ones = 0\n","    \n","    for vote in votes:\n","        if vote == 0:\n","            zeros += 1\n","        else:\n","            ones += 1\n","    \n","    return 1 if ones > zeros else 0\n"," \n","def get_model_accuracy(cm):\n","    return (cm[0][0] + cm[1][1]) / (cm[0][0] + cm[0][1] + cm[1][0] + cm[1][1])\n"," \n","classifiers = build_ensemble_classifier(X_train, y_train)\n","cm = confusion_matrix(y_test, ensemble_classifier_predict(X_test, classifiers))\n"," \n","print(f'accuracy: {get_model_accuracy(cm)}')\n","print(cm)\n","'''\n","'''\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","\n","def models(X, y):\n","    clf1 = RandomForestClassifier()\n","    clf1.fit(X, y)\n","    \n","    clf2 = GaussianNB()\n","    clf2.fit(X, y)\n","    \n","    clf3 = DecisionTreeClassifier()\n","    clf3.fit(X, y)\n","    \n","    clf4 = LogisticRegression()\n","    clf4.fit(X, y)\n","    \n","    clf5 = KNeighborsClassifier()\n","    clf5.fit(X, y)\n","    \n","    clf6 = SVC()\n","    clf6.fit(X, y)\n","\n","    return[clf1, clf2, clf3, clf4, clf5, clf6]\n","\n","def prediction(X, clf):\n","    result = [classifier.predict(X) for classifier in clf]\n","   # print(result)\n","    y_pred = []\n","    for i in range(len(result[0])):\n","        zeros = 0\n","        ones = 0\n","        for j in range(len(result)):\n","            if result[j][i] == 0:\n","                zeros += 1\n","            else:\n","                ones += 1\n","        if zeros != 6 & zeros != 0:        \n","            print(zeros)        \n","        if ones >= zeros:\n","            y_pred.append(1)\n","        else:\n","            y_pred.append(0)\n","\n","clf = models(X_train, y_train)\n","prediction(X_test, clf)\n","'''"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nHello, I just naively combined multiple models into one based on voting and it produces more than 0.8 accuracy.\\n\\nModel used: RandomForest, DecisionTree and NaiveBayes\\n\\ndef build_ensemble_classifier(X, y):\\n    classifier1 = RandomForestClassifier()\\n    classifier1.fit(X, y) \\n    \\n    classifier2 = DecisionTreeClassifier()\\n    classifier2.fit(X, y)\\n \\n    classifier3 = GaussianNB()\\n    classifier3.fit(X, y)\\n    \\n    return [classifier1, classifier2, classifier3]\\n \\ndef ensemble_classifier_predict(X, classifiers):\\n    results = [classifier.predict(X) for classifier in classifiers]\\n    processed_results = []\\n    \\n    for i in range(len(results[0])):\\n        votes = []\\n        \\n        for j in range(len(results)):\\n            votes.append(results[j][i])\\n            \\n        processed_results.append(decide_voting(votes))\\n                \\n    return np.array(processed_results)\\n \\ndef decide_voting(votes):\\n    zeros = 0\\n    ones = 0\\n    \\n    for vote in votes:\\n        if vote == 0:\\n            zeros += 1\\n        else:\\n            ones += 1\\n    \\n    return 1 if ones > zeros else 0\\n \\ndef get_model_accuracy(cm):\\n    return (cm[0][0] + cm[1][1]) / (cm[0][0] + cm[0][1] + cm[1][0] + cm[1][1])\\n \\nclassifiers = build_ensemble_classifier(X_train, y_train)\\ncm = confusion_matrix(y_test, ensemble_classifier_predict(X_test, classifiers))\\n \\nprint(f'accuracy: {get_model_accuracy(cm)}')\\nprint(cm)\\n\""]},"metadata":{"tags":[]},"execution_count":8}]}]}